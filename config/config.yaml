# QA System Configuration

# Security & API Configuration
# Note: These values are loaded from environment variables
SECURITY:
  API_KEY: ${API_KEY}  # Loaded from .env
  GOOGLE_CLOUD_PROJECT: ${GOOGLE_CLOUD_PROJECT}  # Loaded from .env
  GOOGLE_APPLICATION_CREDENTIALS: ${GOOGLE_APPLICATION_CREDENTIALS}  # Loaded from .env
  AUTH_REQUIRED: true

# Vector Database Configuration
VECTOR_STORE:
  TYPE: "chroma"  # Using ChromaDB as the vector store implementation
  PERSIST_DIRECTORY: "./data/vector_store"
  COLLECTION_NAME: "qa_documents"
  # Distance metric used to calculate similarity between queries and documents
  # Options:
  #   "cosine" - Measures cosine similarity between vectors (most common)
  #   "euclidean" - Measures straight-line distance between vectors
  #   "dot" - Measures dot product between vectors
  # Cosine similarity is preferred as it's less sensitive to document length
  # Default: "cosine"
  DISTANCE_METRIC: "cosine"

  # Number of most similar documents to retrieve initially
  # This is the first-pass retrieval before applying relevance scoring
  # Higher values cast a wider net but increase processing time
  # Lower values are faster but might miss relevant documents
  # Range: 1-100, Default: 10
  TOP_K: 40

  # Maximum number of tokens (words/characters) allowed in the generated response
  # Higher values allow for longer, more detailed responses but may take longer to generate
  # Range: 1-4096, Default: 2048
  MAX_TOKENS: 2048

  EMBEDDING_DIM: 768  # This is the maximum dimension for Gemini's embedding model

QUERY_ENGINE:
  MODEL: "models/gemini-2.5-pro-preview-03-25"

  # Controls the randomness/creativity of the response
  # Lower values (0.0-0.3) are more focused and deterministic
  # Higher values (0.7-1.0) are more creative but potentially less precise
  # Range: 0.0-1.0, Default: 0.3
  TEMPERATURE: 0.3

  # Controls the cumulative probability cutoff for token selection
  # Affects the diversity of generated responses
  # Lower values are more focused, higher values allow more variety
  # Range: 0.0-1.0, Default: 0.95
  TOP_P: 0.95

  # Determines how the system formats its responses
  # Options:
  #   "comprehensive" - Detailed responses with context and explanations
  #   "concise" - Brief, direct answers
  #   "bullet_points" - Organized as bullet points
  RESPONSE_MODE: "comprehensive"

  # Minimum similarity score required for a document to be considered relevant
  # Higher values (e.g., 0.8) mean only very close matches are included
  # Lower values (e.g., 0.4) allow more contextual information but may include less relevant matches
  # Range: 0.0-1.0, Default: 0.4
  MIN_RELEVANCE_SCORE: 0.4

  # Maximum number of relevant documents to include in the context when answering
  # Higher values provide more context but may slow down response generation
  # Lower values are faster but might miss relevant information
  # Range: 1-50, Default: 20
  MAX_CONTEXT_DOCS: 20

  # Whether to expand the user's query to improve search results
  # When true: System will try to add relevant terms to improve search accuracy
  # When false: System will use the exact query as provided
  # Default: false
  ENABLE_QUERY_EXPANSION: true  # Enable query expansion for better search results

# Document Processing
DOCUMENT_PROCESSING:
  # Default path for documents to be processed and indexed
  DOCUMENT_PATH: "./docs"

  # List of allowed file extensions for document processing
  # Only files with these extensions will be processed
  # Extensions should be specified without the dot
  ALLOWED_EXTENSIONS:
    - "txt"
    - "md"
    - "rst"
    - "pdf"
    - "doc"
    - "docx"
    - "rtf"
    - "html"
    - "htm"
    - "png"
    - "jpg"
    - "jpeg"
    - "gif"
    - "webp"
    - "bmp"
    - "csv"

  # Maximum size of each text chunk when splitting documents
  # Documents are split into smaller chunks for better processing and retrieval
  # Larger chunks retain more context but may be less precise
  # Smaller chunks are more precise but may lose context
  # Range: 100-2000, Default: 1000 characters
  MAX_CHUNK_SIZE: 1500

  # Number of characters to overlap between consecutive chunks
  # Helps maintain context across chunk boundaries
  # Example: With size 1000 and overlap 200:
  #   Chunk 1: [0-1000], Chunk 2: [800-1800], etc.
  # Higher values ensure better context but increase storage needs
  # Recommended: 10-20% of MAX_CHUNK_SIZE
  # Range: 0-500, Default: 200 characters
  CHUNK_OVERLAP: 300

  # Number of parallel processing tasks for document indexing
  # Higher values speed up processing but use more system resources
  # Should be set based on available CPU cores
  # For 4-core CPU: 4 is optimal
  # For 8-core CPU: 6-8 is optimal
  # Range: 1-16, Default: 4
  CONCURRENT_TASKS: 6

  # Number of documents to process in each batch
  # Affects memory usage and processing efficiency
  # Larger batches are more efficient but use more memory
  # Smaller batches use less memory but take longer
  # Range: 1-100, Default: 10
  BATCH_SIZE: 50
  EXCLUDE_PATTERNS:
    - "**/.*/**"  # Hidden files and directories
    - "**/Excalidraw/**"
    - "**/smart-chats/**"
    - "**/stylesheets/**"
    - "**/tags/**"

# Logging Configuration
LOGGING:
  LEVEL: "INFO"
  FORMAT: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  DATE_FORMAT: "%Y-%m-%d %H:%M:%S"
  LOG_FILE: "logs/qa_system.log"
  ENABLE_DEBUG: false

# Embedding Model Configuration
EMBEDDING_MODEL:
  # Type of embedding model to use
  # Options:
  #   "openai" - OpenAI's text-embedding-ada-002 model
  #   "huggingface" - HuggingFace models
  #   "local" - Local models
  #   "gemini" - Google's Gemini embedding model
  TYPE: "gemini"
  
  # Model name for Gemini embeddings
  # Using text-embedding-004 model
  # Rate limits (Free Tier):
  #   - 60 requests per minute (RPM)
  #   - 1000 requests per day (RPD)
  # Source: https://ai.google.dev/gemini-api/docs/rate-limits#free-tier
  MODEL_NAME: "models/text-embedding-004"
  
  # Batch size for processing multiple texts at once
  # Higher values are more efficient but use more memory
  # Can be higher now due to increased rate limits
  # Range: 1-20, Default: 10
  BATCH_SIZE: 15
  
  # Maximum length of text to embed (in tokens)
  # Texts longer than this will be truncated
  # For text-embedding-004: max 3072 tokens
  MAX_LENGTH: 3072
  
  # Output embedding dimensions
  # text-embedding-004 supports flexible dimensions:
  # Options: 768, 1024
  # Choose based on your needs:
  # - 1024: Higher quality but more storage/compute intensive
  # - 768: Good balance of quality and resource usage
  DIMENSIONS: 768
  
  # Gemini API Rate Limiting Configuration
  # Based on Free Tier limits from https://ai.google.dev/gemini-api/docs/rate-limits#free-tier
  RATE_LIMITS:
    MAX_REQUESTS_PER_MINUTE: 60     # text-embedding-004 Free Tier RPM limit
    MAX_REQUESTS_PER_DAY: 1000      # text-embedding-004 Free Tier RPD limit
    MAX_CONCURRENT_REQUESTS: 10     # Increased for better throughput while staying safe
    BACKOFF_FACTOR: 1.2            # Gentler backoff for faster recovery
    MAX_RETRIES: 3                 # Reduced retries to fail faster if persistent issues
    INITIAL_RETRY_DELAY: 1         # Start with minimal delay
    MAX_RETRY_DELAY: 10           # Shorter max delay for faster recovery
    QUOTA_THRESHOLD: 0.95         # Use more of available quota before throttling
    COOLDOWN_PERIOD: 20          # Shorter cooldown for faster recovery
    QUOTA_TRACKING_FILE: ".quota_usage.json"  # Track quota usage across runs

# Application Settings
APP:
  BASE_DIR: "."
  TIMEOUT: 30
  MAX_RETRIES: 3
  API_PORT: 8000 